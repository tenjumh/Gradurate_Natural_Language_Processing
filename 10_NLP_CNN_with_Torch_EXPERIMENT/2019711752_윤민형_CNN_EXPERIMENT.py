# -*- coding: utf-8 -*-
"""Torch_NLP_CNN_ver1.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FqY0vLEXGI_-MDuDP42JHA7TS0htuWN1
"""

!pip install PyKomoran

import json
from PyKomoran import *
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, fbeta_score
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
import random

def set_seed():
  random.seed(777)
  np.random.seed(777)
  torch.manual_seed(777)
  if torch.cuda.is_available():
    torch.cuda.manual_seed_all(777)

set_seed()

# Train Data 와 Test Data 의 발화를 형태소 분석
komoran = Komoran("EXP")

with open('SpeechAct_tr.json', 'r', encoding='utf-8') as read_file:
    data_tr = json.load(read_file)
with open('SpeechAct_te.json', 'r', encoding='utf-8') as read_file:
    data_te = json.load(read_file)

docs_tr = []
words_tr = []
voca_tr = []
for key in data_tr.keys():
    for message in data_tr[key]:
        komoran_text = komoran.get_plain_text(message[1])
        komoran_text = komoran_text.split(' ')
        for voc in komoran_text:
          voca_tr.append(voc)
        #print(komoran_text)
        words_tr.append(komoran_text)
    docs_tr.append(words_tr)

docs_te = []
words_te = []
for key in data_te.keys():
    for message in data_te[key]:
        komoran_text = komoran.get_plain_text(message[1])
        komoran_text = komoran_text.split(' ')
        words_te.append(komoran_text)
    docs_te.append(words_te)

print(len(docs_tr), len(docs_te))
print(len(words_tr), len(words_te))
print(len(voca_tr))

# 형태소 분석한 Train Data 의 발화를 이용하여 word2idx 구축
words_dic = sorted(set(voca_tr))
word2index = {}
word2index['<PAD>'] = 0
word2index['<UKN>'] = 1
for idx, word in enumerate(words_dic):
  word2index[word] = idx + 2

# Train data label -> label2idx구축
label2idx = {'opening' : 0, 'request' : 1, 'wh-question' : 2, 'yn-question' : 3,
             'inform' : 4, 'affirm' : 5, 'ack' : 6, 'expressive' : 7}

def labellist(path):
  with open(path, 'r', encoding='utf-8') as read_file:
    data = json.load(read_file)

  label_list = []
  for idx, labels in enumerate(data.values()):
    for label in labels:
      label_list.append(label2idx[label[2]])
  return label_list

label_list_tr = labellist('SpeechAct_tr.json')
label_list_te = labellist('SpeechAct_te.json')

# emd = nn.Embedding(num_embeddings = len(word2index) ,embedding_dim = 128)
word_list_tr = []
for key in data_tr:
  if 0 < len(data_tr[key]):
    sentence_list = [i[1] for i in data_tr[key]]
    for sentence in sentence_list:
      index_word = []
      if sentence:
        o_word = komoran.get_plain_text(sentence)
        for word in o_word.split(' '):
          index_word.append(word2index[word])      
      word_list_tr.append(index_word)

word2index.keys()

# emd = nn.Embedding(num_embeddings = len(word2index) ,embedding_dim = 128)
word_list_te = []
for key in data_te:
  sentence_list = [i[1] for i in data_te[key]]
  for sentence in sentence_list:
    index_word = []
    if sentence:
      o_word = komoran.get_plain_text(sentence)
      for word in o_word.split(' '):
        if word not in word2index.keys():
          index_word.append(word2index['<UKN>'])
        else:
          index_word.append(word2index[word])
    word_list_te.append(index_word)

print(len(word_list_tr))
print(len(label_list_tr))
print(len(word_list_te))
print(len(label_list_te))

# list를 torch.tensor로 만드려면 list의 길이가 같아야 함 그래서 0으로 패딩
max_len = 50
word_list_tr_ = np.array([i + [0] * (max_len - len(i)) for i in word_list_tr])
word_list_te_ = np.array([i + [0] * (max_len - len(i)) for i in word_list_te])

word_list_tr_[:5]

label_list_tr[:4]

word_tensor_tr = torch.tensor(word_list_tr_)
label_tensor_tr = torch.tensor(label_list_tr)
word_tensor_te = torch.tensor(word_list_te_)
label_tensor_te = torch.tensor(label_list_te)

print(word_tensor_tr.size(), label_tensor_tr.size())
print(word_tensor_te.size(), label_tensor_te.size())

# 6 번째 과제 , ‘Multi Layer Perceptron 을 활용한 화행 분석 ’ 참조
# 8 번째 과제 , ‘Convolution Neural Networks 를 활용한 화행 분석 (2)’ 참조
epochs = 100
dropout = 0.5
learning_rate = 0.001

class CNN(torch.nn.Module):
  def __init__(self, vocab_size, num_labels):
    super(CNN, self).__init__()
    self.word_embed = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=128, padding_idx=0)
    
    self.conv1 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=3, stride=1)
    self.conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, stride=1)
    self.relu = nn.ReLU()
    self.maxp1 = nn.MaxPool1d(3)
    self.maxp2 = nn.MaxPool1d(12)
    
    self.dropout = nn.Dropout(dropout)
    self.fc1 = nn.Linear(16 * 3, num_labels, bias=True)

  def forward(self, inputs):
    embedded = self.word_embed(inputs).permute(0, 2, 1)
    x = self.maxp1(self.relu(self.conv1(embedded)))
    x = self.maxp2(self.relu(self.conv2(x))).squeeze(2)
    x = self.dropout(torch.cat((x,x,x), dim = 1))

    pred = self.fc1(x)

    return pred

# GPU 가능 여부 및 선택
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# train_tfidf_tensor shape = (발화 수, tfidf_size)
model = CNN(vocab_size = len(word2index), num_labels=8)

# model을 GPU로 이동
model.to(device)

# Train data를 이용 CNN 모델 학습
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# TensorDataset을 이용하여 Input/output data를 하나로 묶음
Train_dataset = torch.utils.data.TensorDataset(word_tensor_tr, label_tensor_tr)
Test_dataset = torch.utils.data.TensorDataset(word_tensor_te, label_tensor_te)

# DataLoader를 선언하고 batch size 만큼 데이터를 가지고 와서 학습
# Shuffle 여부 결정
train_DataLoader = torch.utils.data.DataLoader(Train_dataset, shuffle=True, batch_size=4)
test_DataLoader = torch.utils.data.DataLoader(Test_dataset, shuffle=False, batch_size=1)

model.train(True)
model.zero_grad()
for epoch in range(epochs):
  epoch_loss = 0
  for batch in train_DataLoader:
    # batch : (tfidf data, label)
    batch = tuple(t.to(device) for t in batch)
    y_pred = model(batch[0])

    loss = criterion(y_pred, batch[1])
    epoch_loss += loss.item()

    loss.backward()
    optimizer.step()
    model.zero_grad()
  if (epoch+1) % 10 == 0:
    print(epoch, epoch_loss)
model.train(False)

'''
# Test model
model.eval()
with torch.no_grad():
  correct = 0
  total = 0
  for images, labels in test_DataLoader:
    images = images.to(device)
    labels = labels.to(device)
    outputs = model(images)
    _, pred = torch.max(outputs.data, 1)
    total += labels.size(0)
    correct += (pred == labels).sum().item()

  print('Test Accuracy of the model on the  : {}'.format(100*correct/total))
'''

# Test
model.eval()
pred = None
label = None
for batch in test_DataLoader:
  # batch : [tfidf_data, label]
  batch = tuple(t.to(device) for t in batch)

  # gradient를 계산하지 않도록 선언
  with torch.no_grad():
    y_pred = model(batch[0])

  if pred is None:
    pred = y_pred.detach().cpu().numpy()
    label = batch[1].detach().cpu().numpy()
  else:
    pred = np.append(pred, y_pred.detach().cpu().numpy(), axis=0)
    label = np.append(label, batch[1].detach().cpu().numpy(), axis=0)

pred = np.argmax(pred, axis=1)

if len(label_tensor_te) == len(pred):
  print("True")

def eval(true, pred):
    ave = ['macro', 'micro']
    precision = []
    recall = []
    fbeta = []
    f1 = []
    acc = accuracy_score(true, pred)
    for i in ave:
        precision.append(precision_score(true, pred, average=i))
        recall.append(recall_score(true, pred, average=i))
        f1.append(f1_score(true, pred, average=i))
    return acc, precision, recall, f1

conf_mat = confusion_matrix(label_tensor_te, pred)

evaluation = eval(label_tensor_te, pred)

evaluation

save_file_name = '2019711752_윤민형_CNN_EXPERIMENT_3'
with open('./'+save_file_name+'.txt', 'w', encoding='utf-8', newline='') as writer_text:
    list = ['precision', 'recall', 'f1-score']
    writer_text.writelines('epochs : ' + str(epochs) +'\n' + 'dropout : ' + str(dropout) + '\n' + 'learning_rate : ' + str(learning_rate) + '\n\n')
    for idx, k in enumerate(range(len(evaluation[1:]))):
        a = np.round(evaluation[k+1][0]*100, 4)
        b = np.round(evaluation[k+1][1]*100, 4)
        writer_text.writelines('Macro average ' + str(list[idx]) +' : ' + str(a) +'%' + '\n')
        writer_text.writelines('Micro averate ' + str(list[idx]) +' : ' + str(b) +'%'+ '\n\n')
    #writer_text.close()
    print("[저장 완료]")

