{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sckiti-Learn\n",
    "### 1. DictVectorizer\n",
    "- 문서에서 단어의 사용빈도를 나타내는 딕셔너리 정보를 입력 받아 BOW인코딩한 수치 벡터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "Bag_Of_Word = DictVectorizer(sparse=False)\n",
    "Frequency = [{'바나나/NNP' : 1}, {'먹/VV' : 1, '사과/NNP' : 1}]\n",
    "output = Bag_Of_Word.fit_transform(Frequency)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CountVectorizer\n",
    "- 문서를 토큰 리스트로 변환\n",
    "- 각 문서에서 토큰의 출현 빈도를 센다.\n",
    "- 각 문서를 BOW 인코딩 벡터로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['수박 사과', '먹 사과', '바나나', '바나나']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from PyKomoran import *\n",
    "\n",
    "komoran = Komoran(\"EXP\")\n",
    "\n",
    "corpus = ['수박과 사과',\n",
    "         '먹고 싶은 사과',\n",
    "         '길고 노란 바나나',\n",
    "         '노란 바나나']\n",
    "\n",
    "POS_corpus = list()\n",
    "# 형태소 분석 후 \"NNG\", \"NNP\", \"VV\"태그인 Token만 구분자(띄어쓰기) 단위로 POS_corpus 리스트에 추가\n",
    "for sentence in corpus:\n",
    "    POS_corpus.append(' '.join(komoran.get_morphes_by_tags(sentence, tag_list=[\"NNG\", \"NNP\", \"VV\"])))\n",
    "    \n",
    "print(POS_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'수박': 3, '사과': 2, '먹': 0, '바나나': 1}\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "vect.fit_transform(POS_corpus)\n",
    "print(vect.vocabulary_)   # POS의 딕셔너리 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1, 1], [1, 0, 1, 0], [0, 1, 0, 0], [0, 1, 0, 0]]\n",
      "[[0 0 1 1]\n",
      " [1 0 1 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(POS_corpus).toarray().tolist())  # 해당 딕셔너리 순번에 문서별로 카운팅\n",
    "print(vect.transform(POS_corpus).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TfidVectorizer\n",
    "- TfidVectorizer는 TF-IDF로 벡터를 만들 때, L2 Norm으로 정규화 한다.\n",
    "- L2 Norm : ||x||2 : 모든 x를 제곱합하여 루트\n",
    "- TF-IDF를 계산할 때, idf = log( N+1 / df(t)+1 ) + 1\n",
    "- 1) N+1 / df(t)+1를 사용하는 이유 : 모든 토큰을 한 번 포함하는 추가 문서가 있는 것처럼 하여 \"문서빈도\"와 \"문서의 수\"에 1을 더하여 0으로 나눠주는 경우가 없도록...\n",
    "- 2) log항에 1을 더하는 이유 : idf값이 0이라면 가중치의 역할을 할 수 없으므로 1을 더해주 최소 1이상의 값을 가지게..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['수박 사과', '먹 사과', '바나나', '바나나']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PyKomoran import *\n",
    "\n",
    "komoran = Komoran(\"EXP\")\n",
    "\n",
    "corpus = ['수박과 사과',\n",
    "         '먹고 싶은 사과',\n",
    "         '길고 노란 바나나',\n",
    "         '노란 바나나']\n",
    "\n",
    "POS_corpus = list()\n",
    "# 형태소 분석 후 \"NNG\", \"NNP\", \"VV\"태그인 Token만 구분자(띄어쓰기) 단위로 POS_corpus 리스트에 추가\n",
    "for sentence in corpus:\n",
    "    POS_corpus.append(' '.join(komoran.get_morphes_by_tags(sentence, tag_list=[\"NNG\", \"NNP\", \"VV\"])))\n",
    "    \n",
    "print(POS_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'수박': 3, '사과': 2, '먹': 0, '바나나': 1}\n"
     ]
    }
   ],
   "source": [
    "tfidfvect = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "tfidfvect.fit_transform(POS_corpus)\n",
    "print(tfidfvect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.6191302964899972, 0.7852882757103967], [0.7852882757103967, 0.0, 0.6191302964899972, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]]\n",
      "[[0.         0.         0.6191303  0.78528828]\n",
      " [0.78528828 0.         0.6191303  0.        ]\n",
      " [0.         1.         0.         0.        ]\n",
      " [0.         1.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidfvect.transform(POS_corpus).toarray().tolist())\n",
    "print(tfidfvect.transform(POS_corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
