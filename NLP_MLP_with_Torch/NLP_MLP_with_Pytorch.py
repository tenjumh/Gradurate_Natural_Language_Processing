# -*- coding: utf-8 -*-
"""2019711752_윤민형_MLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15yaXQA0ep06BbyF0z07e2ThLp_ImaFez
"""

!pip install PyKomoran

import json
from PyKomoran import *
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, fbeta_score
import torch
import numpy as np
import random

def set_seed():
  random.seed(777)
  np.random.seed(777)
  torch.manual_seed(777)
  if torch.cuda.is_available():
    torch.cuda.manual_seed_all(777)

set_seed()

label_list = ['opening', 'request', 'wh-question', 'yn-question', 'inform', 'affirm', 'ack', 'expressive']
label_map = {label: i for i, label in enumerate(label_list)}

komoran = Komoran("EXP")

with open('./SpeechAct_tr.json', 'r', encoding='utf-8') as read_file:
    data_tr = json.load(read_file)

tfidf_word_tr = []
tfidf_label_tr = []
for key_tr in data_tr:
  if data_tr[key_tr] == 0:
    continue
  for message_tr in range(len(data_tr[key_tr])):
    komoran_text_tr = ' '.join(komoran.get_morphes_by_tags(data_tr[key_tr][message_tr][1],
                                                        tag_list=['NNP', 'NNG', 'VV']))
    tfidf_word_tr.append(komoran_text_tr)
    komoran_label_tr = data_tr[key_tr][message_tr][2]
    tfidf_label_tr.append(komoran_label_tr)

tfidfvect_tr = TfidfVectorizer(token_pattern='(?u)\\b\\w+\\b', stop_words=None)
tfidfvect_tr.fit_transform(tfidf_word_tr)

train_label_list = []
for i in tfidf_label_tr:
  if i in label_map.keys():
    train_label_list.append(label_map[i])
train_tfidf_list = tfidfvect_tr.transform(tfidf_word_tr).toarray().tolist()

with open('./SpeechAct_te.json', 'r', encoding='utf-8') as read_file:
    data_te = json.load(read_file)

tfidf_word_te = []
tfidf_label_te = []
for key_te in data_te:
  if data_te[key_te] == 0:
    continue
  for message_te in range(len(data_te[key_te])):
    komoran_text_te = ' '.join(komoran.get_morphes_by_tags(data_te[key_te][message_te][1],
                                                        tag_list=['NNP', 'NNG', 'VV']))
    tfidf_word_te.append(komoran_text_te)
    komoran_label_te = data_te[key_te][message_te][2]
    tfidf_label_te.append(komoran_label_te)

tfidfvect_te = TfidfVectorizer(token_pattern='(?u)\\b\\w+\\b', stop_words=None)
tfidfvect_te.fit_transform(tfidf_word_tr)

test_label_list = []
for i in tfidf_label_te:
  if i in label_map.keys():
    test_label_list.append(label_map[i])
test_tfidf_list = tfidfvect_te.transform(tfidf_word_te).toarray().tolist()

print(len(train_tfidf_list), len(train_label_list))
print(len(test_tfidf_list), len(test_label_list))

train_tfidf_tensor = torch.tensor(train_tfidf_list)
train_label_tensor = torch.tensor(train_label_list)
test_tfidf_tensor = torch.tensor(test_tfidf_list)
test_label_tensor = torch.tensor(test_label_list)

print(train_tfidf_tensor.shape)
print(train_label_tensor.shape)
print(test_tfidf_tensor.shape)
print(test_label_tensor.shape)

class Perceptron(torch.nn.Module):
  def __init__(self, tfidf_size, num_label):
    super(Perceptron, self).__init__()
    self.linear = torch.nn.Linear(tfidf_size, num_label)

  def forward(self, tfidf_input):
    y_pred = self.linear(tfidf_input)

    return y_pred

# GPU 가능 여부 및 선택
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# train_tfidf_tensor shape = (발화 수, tfidf_size)
model = Perceptron(tfidf_size = train_tfidf_tensor.shape[1], num_label=len(label_list))

# model을 GPU로 이동
model.to(device)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# TensorDataset을 이용하여 Input/output data를 하나로 묶음
Train_dataset = torch.utils.data.TensorDataset(train_tfidf_tensor, train_label_tensor)
Test_dataset = torch.utils.data.TensorDataset(test_tfidf_tensor, test_label_tensor)

# DataLoader를 선언하고 batch size 만큼 데이터를 가지고 와서 학습
# Shuffle 여부 결정
train_DataLoader = torch.utils.data.DataLoader(Train_dataset, shuffle=True, batch_size=4)
test_DataLoader = torch.utils.data.DataLoader(Test_dataset, shuffle=False, batch_size=1)

# Train
model.train(True)
model.zero_grad()
for epoch in range(500):
  epoch_loss = 0
  for batch in train_DataLoader:
    # batch : (tfidf data, label)
    batch = tuple(t.to(device) for t in batch)
    y_pred = model(batch[0])

    loss = criterion(y_pred, batch[1])
    epoch_loss += loss.item()

    loss.backward()
    optimizer.step()
    model.zero_grad()
  if (epoch+1) % 10 == 0:
    print(epoch, epoch_loss)
model.train(False)

# Test
model.eval()
pred = None
label = None
for batch in test_DataLoader:
  # batch : [tfidf_data, label]
  batch = tuple(t.to(device) for t in batch)

  # gradient를 계산하지 않도록 선언
  with torch.no_grad():
    y_pred = model(batch[0])

  if pred is None:
    pred = y_pred.detach().cpu().numpy()
    label = batch[1].detach().cpu().numpy()
  else:
    pred = np.append(pred, y_pred.detach().cpu().numpy(), axis=0)
    label = np.append(label, batch[1].detach().cpu().numpy(), axis=0)

pred = np.argmax(pred, axis=1)

if len(test_label_tensor) == len(pred):
  print("True")

def eval(true, pred):
    ave = ['macro', 'micro']
    precision = []
    recall = []
    fbeta = []
    f1 = []
    acc = accuracy_score(true, pred)
    for i in ave:
        precision.append(precision_score(true, pred, average=i))
        recall.append(recall_score(true, pred, average=i))
        f1.append(f1_score(true, pred, average=i))
    return acc, precision, recall, f1

conf_mat = confusion_matrix(test_label_tensor, pred)

evaluation = eval(test_label_tensor, pred)

evaluation

save_file_name = '2019711752_윤민형_MLP'
with open('./'+save_file_name+'.txt', 'w', encoding='utf-8', newline='') as writer_text:
    list = ['precision', 'recall', 'f1-score']
    for idx, k in enumerate(range(len(evaluation[1:]))):
        writer_text.writelines('Macro average ' + str(list[idx]) +' : ' 
                               + str(evaluation[k+1][0]*100) +'%' + '\n')
        writer_text.writelines('Micro averate ' + str(list[idx]) +' : ' 
                               + str(evaluation[k+1][1]*100) +'%'+ '\n\n')
    #writer_text.close()
    print("[저장 완료]")

